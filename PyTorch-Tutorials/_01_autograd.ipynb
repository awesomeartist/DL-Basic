{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0d820fdb209f5fb3d7158d20fb3da7b4f5ac073d878102351ac8f6edafea36c44",
   "display_name": "Python 3.8.8 64-bit ('yolov5': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 自动差分引擎 autograd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于此示例，我们从torchvision加载了经过预训练的 resnet18 模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&宽度为 64，其对应的label初始化为一些随机值。\n",
    "\n",
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "\n",
    "\n",
    "#接下来，我们通过模型的每一层运行输入数据以进行预测。 这是正向传播。\n",
    "prediction = model(data) # forward pass\n",
    "\n",
    "\n",
    "#我们使用模型的预测和相应的标签来计算误差（loss）。 下一步是通过网络反向传播此误差。 当我们在误差张量上调用.backward()时，开始反向传播。 然后，Autograd 会为每个模型参数计算梯度并将其存储在参数的.grad属性中。\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass\n",
    "\n",
    "\n",
    "#接下来，我们加载一个优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "\n",
    "#最后，我们调用.step()启动梯度下降。 优化器通过.grad中存储的梯度来调整每个参数。\n",
    "optim.step() #gradient descent\n"
   ]
  },
  {
   "source": [
    "## Autograd 的微分\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True) #要求跟踪a,b的操作\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2   #Q为误差\n",
    "\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#生成一个内容为[2,3]的张量，Varibale 默认是不要求梯度的，如果要求梯度，\n",
    "#需要加上requires_grad=True来说明\n",
    "#这里的Variable是为了设置变量，把a0=2,a1=3设置为两个变量\n",
    "a = Variable(torch.tensor([2.,3.]),requires_grad=True)\n",
    "b = a+3\n",
    "c = b*3\n",
    "out=c.mean() #求均值\n",
    "out.backward()\n",
    "print(\"a=\",a)\n",
    "print(\"out=\",out)\n",
    "print(a.grad)  #求out对a的偏导\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#生成一个内容为[2,4]的张量，Varibale 默认是不要求梯度的，如果要求梯度，\n",
    "#需要加上requires_grad=True来说明\n",
    "a = Variable(torch.Tensor([[2,4]]),requires_grad=True)\n",
    "b = torch.zeros(1,2)\n",
    "b[0,0] = a[0,0]**2+a[0,1]\n",
    "b[0,1] = a[0,1]**3+a[0,0]\n",
    "out = 2*b\n",
    "\n",
    "#括号里面的参数要传入和out维度一样的矩阵\n",
    "#这个矩阵里面的元素会作为最后加权输出的权重系数\n",
    "out.backward(torch.FloatTensor([[1, 0]]),retain_graph=True)\n",
    "A_temp = a.grad.clone()\n",
    "a.grad.zero_()\n",
    "out.backward(torch.FloatTensor([[0, 1]]))\n",
    "B_temp = a.grad\n",
    "print('jacobian matrix is:')\n",
    "print(torch.cat((A_temp, B_temp)))\n",
    "\n",
    "# out.backward(torch.FloatTensor([[1,2]]))\n",
    "# print(\"a=\",a)\n",
    "# print(\"out=\",out)\n",
    "# print(a.grad)  #求out对a的偏导\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#生成一个内容为[2,3]的张量，Varibale 默认是不要求梯度的，如果要求梯度，\n",
    "#需要加上requires_grad=True来说明\n",
    "a = Variable(torch.Tensor([[2,3],[1, 2]]),requires_grad=True)\n",
    "w = Variable(torch.ones(2,2),requires_grad=True)\n",
    "out = torch.mm(a,w)\n",
    "\n",
    "#括号里面的参数要传入和out维度一样的矩阵\n",
    "#这个矩阵里面的元素会作为最后加权输出的权重系数\n",
    "out.backward(torch.FloatTensor([[1,1],[1,1]]))\n",
    "print(\"gradients are:{}\".format(w.grad.data))\n"
   ]
  },
  {
   "source": [
    "## 从 DAG 中排除\n",
    ">torch.autograd跟踪所有将其requires_grad标志设置为True的张量的操作。 对于不需要梯度的张量，将此属性设置为False会将其从梯度计算 DAG 中排除。\n",
    "\n",
    ">即使只有一个输入张量具有requires_grad=True，操作的输出张量也将需要梯度。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients? : {b.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}